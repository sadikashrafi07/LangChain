{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face x LangChain : A new partner package in LangChain\n",
    "langchain_huggingface, a partner package in LangChain jointly maintained by Hugging Face and LangChain. This new Python package is designed to bring the power of the latest development of Hugging Face into LangChain and keep it up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_huggingface in e:\\udemy final\\langchain\\venv\\lib\\site-packages (0.0.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from langchain_huggingface) (0.23.4)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.52 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from langchain_huggingface) (0.2.10)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from langchain_huggingface) (3.0.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from langchain_huggingface) (0.19.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from langchain_huggingface) (4.41.2)\n",
      "Requirement already satisfied: filelock in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.1)\n",
      "Requirement already satisfied: requests in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.1.77)\n",
      "Requirement already satisfied: pydantic<3,>=1 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (1.10.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (8.4.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.3.1)\n",
      "Requirement already satisfied: numpy in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.5.0)\n",
      "Requirement already satisfied: scipy in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
      "Requirement already satisfied: Pillow in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (10.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (2024.5.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain_huggingface) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (3.10.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.6.2)\n",
      "Requirement already satisfied: sympy in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.12.1)\n",
      "Requirement already satisfied: networkx in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.3)\n",
      "Requirement already satisfied: jinja2 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.4)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2021.4.0)\n",
      "Requirement already satisfied: colorama in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain_huggingface) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2021.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in e:\\udemy final\\langchain\\venv\\lib\\site-packages (0.23.4)\n",
      "Requirement already satisfied: filelock in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface_hub) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface_hub) (2024.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface_hub) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from requests->huggingface_hub) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from requests->huggingface_hub) (2024.6.2)\n"
     ]
    }
   ],
   "source": [
    "## API Call\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFaceEndpoint\n",
    "#### How to Access HuggingFace Models with API\n",
    "There are also two ways to use this class. You can specify the model with the repo_id parameter. Those endpoints use the serverless API, which is particularly beneficial to people using pro accounts or enterprise hub. Still, regular users can already have access to a fair amount of request by connecting with their HF token in the environment where they are executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\win10\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', temperature=0.7, model_kwargs={'max_length': 150, 'token': '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'}, model='mistralai/Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?\\n\\nMachine learning is a subset of artificial intelligence that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it to learn for themselves.\\n\\nThe process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for patterns in data and make better decisions in the future based on the examples that we provide. The primary aim is to allow the computers to learn automatically without human intervention or assistance and adjust actions accordingly.\\n\\nMachine learning can be categorized into three types:\\n\\n- Supervised Learning: It involves the use of labeled data, where the desired output is known. The goal is to learn a mapping between the input and output variables, so that given a new input, the model can predict the output.\\n- Unsupervised Learning: It is the process of learning without labeled data. The goal is to find the hidden pattern or structure within the data.\\n- Reinforcement Learning: It is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize a reward or minimize a cost.\\n\\nMachine learning is used in a wide range of applications, including image recognition, speech recognition, natural language processing, and recommendation systems.\\n\\nMachine learning is a powerful tool for solving complex problems and making predictions, but it requires a large amount of data, computational resources, and expertise to implement. It is an active area of research with many ongoing developments and advancements.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'❓\\n\\nGenerative AI refers to a type of artificial intelligence that can create new content, such as images, music, or text, based on patterns learned from existing data. Generative AI models use techniques such as deep learning, neural networks, and reinforcement learning to generate new content that is similar to, but not identical to, the data they were trained on. Some examples of generative AI include deepfakes, AI-generated art, and AI-written stories.\\n\\nGenerative AI has a wide range of potential applications, including creating personalized content for users, generating new ideas for designers and creators, and automating tasks such as data entry and content creation. However, generative AI also raises ethical concerns, such as the potential for misuse in creating deepfakes or generating offensive or harmful content. It is important for developers and users of generative AI to be aware of these potential risks and to take steps to mitigate them.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is generative AI \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\win10\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='google/gemma-2-9b', temperature=0.7, model_kwargs={'max_length': 150, 'token': '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'}, model='google/gemma-2-9b', client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>, async_client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"google/gemma-2-9b\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": " (Request ID: fxSl_33ZiXHMdaimyWjqv)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/google/gemma-2-9b.\nIf you are trying to create or update content,make sure you have a token with the `write` role.\nThe model google/gemma-2-9b is too large to be loaded automatically (36GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32me:\\UDemy Final\\Langchain\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32me:\\UDemy Final\\Langchain\\venv\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://api-inference.huggingface.co/models/google/gemma-2-9b",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is machine learning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\UDemy Final\\Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:276\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    274\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 276\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    277\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    278\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    279\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    280\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    281\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    282\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    283\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    284\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    285\u001b[0m         )\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    288\u001b[0m     )\n",
      "File \u001b[1;32me:\\UDemy Final\\Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:633\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    627\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    631\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    632\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\UDemy Final\\Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:803\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    789\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    790\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    791\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    801\u001b[0m         )\n\u001b[0;32m    802\u001b[0m     ]\n\u001b[1;32m--> 803\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    804\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    805\u001b[0m     )\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32me:\\UDemy Final\\Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:670\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    669\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    671\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32me:\\UDemy Final\\Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:657\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    649\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    654\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    656\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 657\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    658\u001b[0m                 prompts,\n\u001b[0;32m    659\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    660\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    661\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    662\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    663\u001b[0m             )\n\u001b[0;32m    664\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    665\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    666\u001b[0m         )\n\u001b[0;32m    667\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    668\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32me:\\UDemy Final\\Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1322\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1321\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1322\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1324\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1325\u001b[0m     )\n\u001b[0;32m   1326\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32me:\\UDemy Final\\Langchain\\venv\\lib\\site-packages\\langchain_huggingface\\llms\\huggingface_endpoint.py:258\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    255\u001b[0m     invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m invocation_params[\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n",
      "File \u001b[1;32me:\\UDemy Final\\Langchain\\venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:273\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 273\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32me:\\UDemy Final\\Langchain\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:367\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[0;32m    361\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf you are trying to create or update content,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake sure you have a token with the `write` role.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m:  (Request ID: fxSl_33ZiXHMdaimyWjqv)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/google/gemma-2-9b.\nIf you are trying to create or update content,make sure you have a token with the `write` role.\nThe model google/gemma-2-9b is too large to be loaded automatically (36GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints)."
     ]
    }
   ],
   "source": [
    "llm.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\win10\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', temperature=0.7, model_kwargs={'max_length': 150, 'token': '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'}, model='mistralai/Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] template='\\nQuestion:{question}\\nAnswer:Lets think step by step.\\n'\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate,LLMChain\n",
    "template=\"\"\"\n",
    "Question:{question}\n",
    "Answer:Lets think step by step.\n",
    "\"\"\"\n",
    "prompt=PromptTemplate(template=template,input_variables=[\"question\"])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? India\\n\\nThe 2011 Cricket World Cup was the 10th Cricket World Cup, and was held in the Indian subcontinent from 19 February to 2 April 2011. The final was played at the Wankhede Stadium in Mumbai, India on 2 April 2011. The tournament was won by India, who defeated Sri Lanka by six wickets with 10 balls remaining. It was India's second World Cup title, and their first since the 1983 World Cup.\\n\\nThe tournament was co-hosted by India, Bangladesh and Sri Lanka, making it the first time that the tournament had been held in three countries. The final was the first time the World Cup final had been held in India. The tournament was also notable for being the first World Cup to feature all ten Test-playing nations, as well as the Associate members Afghanistan and Ireland.\\n\\nThe tournament was won by India, who defeated Sri Lanka by six wickets with 10 balls remaining in the final. It was India's second World Cup title, and their first since the 1983 World Cup. The final was the first time the World Cup final had been held in India.\\n\\nThe tournament was co-hosted by India, Bangladesh and Sri Lanka, making it the first time that the tournament had been held in three countries. The final was the first time the World Cup final had been held in India. The tournament was also notable for being the first World Cup to feature all ten Test-playing nations, as well as the Associate members Afghanistan and Ireland.\\n\\nThe final was a thrilling match, with India chasing a target of 275 set by Sri Lanka. The match went down to the wire, with India requiring 24 runs off the final two overs. Captain Mahendra Singh Dhoni hit a six off the final ball of the penultimate over to bring the required run rate down to 12 runs off the final six balls. Dhoni then hit a four off the first ball of the final over, before hitting a six off the second ball to put India within striking distance of victory.\\n\\nIndia won the match with six wickets and 10 balls remaining, with Dhoni being named the Player of the Match for his unbeaten 91 off \""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain=LLMChain(llm=llm,prompt=prompt)\n",
    "llm.invoke(\"Who won the cricket World up 2011\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\UDemy Final\\Langchain\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = hf.embed_query(\"hi this is harrison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0284165870398283,\n",
       " 0.012183267623186111,\n",
       " 0.027443980798125267,\n",
       " -0.05482868850231171,\n",
       " 0.02423887327313423,\n",
       " 0.0007662874413654208,\n",
       " 0.06783366203308105,\n",
       " 0.01634831726551056,\n",
       " -0.018950754776597023,\n",
       " 0.012542901560664177,\n",
       " 0.02156498283147812,\n",
       " -0.08793040364980698,\n",
       " 0.0006460295990109444,\n",
       " 0.03327083960175514,\n",
       " 0.005463775712996721,\n",
       " -0.060376472771167755,\n",
       " 0.05042264610528946,\n",
       " 0.004434810485690832,\n",
       " 0.00095986551605165,\n",
       " 0.0017405656399205327,\n",
       " 0.0032988341990858316,\n",
       " 0.03167250379920006,\n",
       " -0.04880750551819801,\n",
       " -0.044819176197052,\n",
       " 0.07132110744714737,\n",
       " -0.007510855793952942,\n",
       " -0.001125940354540944,\n",
       " -0.01580117642879486,\n",
       " -0.029402390122413635,\n",
       " -0.17224565148353577,\n",
       " -0.03189520537853241,\n",
       " -0.0016291581559926271,\n",
       " 0.018104972317814827,\n",
       " 0.015315393917262554,\n",
       " -0.020729590207338333,\n",
       " -0.00887296348810196,\n",
       " -0.001282267621718347,\n",
       " 0.027276894077658653,\n",
       " -0.01011426467448473,\n",
       " 0.012621637433767319,\n",
       " -0.007077880669385195,\n",
       " -0.01669319160282612,\n",
       " 0.04085584357380867,\n",
       " 0.02393835410475731,\n",
       " -0.020081529393792152,\n",
       " 0.028681160882115364,\n",
       " -0.01940075308084488,\n",
       " -0.01461819652467966,\n",
       " 0.017379628494381905,\n",
       " 0.004164098296314478,\n",
       " 0.06415650248527527,\n",
       " 0.04768308624625206,\n",
       " 0.0018365210853517056,\n",
       " -8.073195931501687e-05,\n",
       " 0.016596795991063118,\n",
       " 0.0111242039129138,\n",
       " 0.06969437748193741,\n",
       " 0.051820483058691025,\n",
       " 0.05568530783057213,\n",
       " 0.055515408515930176,\n",
       " 0.0005039457464590669,\n",
       " 0.0418706052005291,\n",
       " -0.15344087779521942,\n",
       " 0.05180780962109566,\n",
       " 0.006689797155559063,\n",
       " -0.031670715659856796,\n",
       " -0.00910499133169651,\n",
       " -0.051604729145765305,\n",
       " 0.0425085574388504,\n",
       " 0.028200045228004456,\n",
       " -0.010748116299510002,\n",
       " 0.022405782714486122,\n",
       " 0.04439552128314972,\n",
       " 0.004115514922887087,\n",
       " 0.018998483195900917,\n",
       " -0.004357167519629002,\n",
       " 0.04762762784957886,\n",
       " 0.011824632994830608,\n",
       " 0.008164606057107449,\n",
       " 0.008177308365702629,\n",
       " -0.009698739275336266,\n",
       " -0.014260312542319298,\n",
       " 0.011409704573452473,\n",
       " -0.07362118363380432,\n",
       " -0.054395247250795364,\n",
       " -0.05703963711857796,\n",
       " -0.003608571831136942,\n",
       " 0.002666075713932514,\n",
       " 0.02378244139254093,\n",
       " 0.01537624467164278,\n",
       " -0.07020371407270432,\n",
       " -0.03130035102367401,\n",
       " -0.0031142791267484426,\n",
       " -0.015812193974852562,\n",
       " -0.03791400045156479,\n",
       " -0.025921940803527832,\n",
       " 0.018168412148952484,\n",
       " -0.03882459178566933,\n",
       " -0.05674508213996887,\n",
       " 0.5792059898376465,\n",
       " -0.05278833210468292,\n",
       " 0.020716387778520584,\n",
       " 0.06794392317533493,\n",
       " -0.04541652649641037,\n",
       " 0.011642459779977798,\n",
       " -0.02157176099717617,\n",
       " 0.02034171298146248,\n",
       " -0.027448952198028564,\n",
       " -0.04558897390961647,\n",
       " -0.02944355271756649,\n",
       " -0.0236625075340271,\n",
       " -0.034315261989831924,\n",
       " 0.0019388616783544421,\n",
       " -0.07095140218734741,\n",
       " 0.034556321799755096,\n",
       " -0.03055894933640957,\n",
       " 0.03907858952879906,\n",
       " -0.02970730885863304,\n",
       " -0.0008283004281111062,\n",
       " -0.01215936616063118,\n",
       " -0.018272822722792625,\n",
       " 0.025486508384346962,\n",
       " -0.004461656324565411,\n",
       " 0.016335351392626762,\n",
       " 0.01912648230791092,\n",
       " -0.054832056164741516,\n",
       " 0.027635997161269188,\n",
       " -0.004757694900035858,\n",
       " 0.059001751244068146,\n",
       " -0.0016944559756666422,\n",
       " 0.008014989085495472,\n",
       " -0.03772681951522827,\n",
       " -0.09893042594194412,\n",
       " -0.02257443219423294,\n",
       " -0.037604670971632004,\n",
       " -0.0021698607597500086,\n",
       " 0.0032445944380015135,\n",
       " -0.01920250989496708,\n",
       " -0.008631231263279915,\n",
       " -0.048023100942373276,\n",
       " 0.008696728385984898,\n",
       " -0.09516110271215439,\n",
       " -0.03496047481894493,\n",
       " -0.04360800236463547,\n",
       " -0.000344040832715109,\n",
       " -0.010173690505325794,\n",
       " -0.030999548733234406,\n",
       " 0.024309683591127396,\n",
       " -0.020402006804943085,\n",
       " 0.031139396131038666,\n",
       " 0.0008811206789687276,\n",
       " 0.013916504569351673,\n",
       " -0.03119620494544506,\n",
       " -0.03715404495596886,\n",
       " 0.004029636271297932,\n",
       " 0.014799792319536209,\n",
       " 0.043188951909542084,\n",
       " 0.038754839450120926,\n",
       " 0.013852003961801529,\n",
       " 0.019797898828983307,\n",
       " 0.010267077945172787,\n",
       " -0.005434087943285704,\n",
       " -0.014299239031970501,\n",
       " 0.02763783559203148,\n",
       " 0.009802657179534435,\n",
       " -0.1355028599500656,\n",
       " -0.017139777541160583,\n",
       " 0.017617106437683105,\n",
       " 0.02313225157558918,\n",
       " 0.001759000588208437,\n",
       " 0.03088941052556038,\n",
       " 0.039918724447488785,\n",
       " -0.01368420198559761,\n",
       " 0.02481650374829769,\n",
       " 0.05405018478631973,\n",
       " 0.017761139199137688,\n",
       " -0.018475042656064034,\n",
       " 0.02595539018511772,\n",
       " -0.006377549842000008,\n",
       " -0.01658731698989868,\n",
       " 0.03784802183508873,\n",
       " -0.027290046215057373,\n",
       " -0.0528457835316658,\n",
       " -0.03803316876292229,\n",
       " 0.051911089569330215,\n",
       " -0.007557078264653683,\n",
       " -0.03180529177188873,\n",
       " 0.013284173794090748,\n",
       " -0.027723707258701324,\n",
       " 0.05630655214190483,\n",
       " 0.0030419081449508667,\n",
       " 0.0533248707652092,\n",
       " -0.057911261916160583,\n",
       " -0.011325831525027752,\n",
       " -0.031172024086117744,\n",
       " 0.02560870535671711,\n",
       " 0.033890631049871445,\n",
       " -0.0010284087620675564,\n",
       " 0.015864884480834007,\n",
       " 0.010595192201435566,\n",
       " -0.027037810534238815,\n",
       " -0.0009308484150096774,\n",
       " -0.048152241855859756,\n",
       " 0.028179241344332695,\n",
       " 0.010320613160729408,\n",
       " 0.06662958860397339,\n",
       " -0.016558198258280754,\n",
       " -0.0044313534162938595,\n",
       " 0.03823424503207207,\n",
       " -0.0234081968665123,\n",
       " -0.03558176010847092,\n",
       " -0.05829072371125221,\n",
       " -0.011181517504155636,\n",
       " -0.017684565857052803,\n",
       " -0.016141293570399284,\n",
       " -0.0342453271150589,\n",
       " -0.025139562785625458,\n",
       " 0.03939666971564293,\n",
       " -0.023658234626054764,\n",
       " -0.0077250078320503235,\n",
       " -0.0050989119336009026,\n",
       " -0.03523438051342964,\n",
       " -0.014076831750571728,\n",
       " -0.2232603132724762,\n",
       " -0.031471364200115204,\n",
       " -0.0012906035408377647,\n",
       " -0.0017200004076585174,\n",
       " -0.007846039719879627,\n",
       " -0.058023251593112946,\n",
       " 0.0461745485663414,\n",
       " 0.024552656337618828,\n",
       " 0.07320841401815414,\n",
       " 0.01726832613348961,\n",
       " 0.04761204496026039,\n",
       " 0.01347331516444683,\n",
       " -0.00551601080223918,\n",
       " -0.01435784064233303,\n",
       " -0.009674331173300743,\n",
       " 0.04878250136971474,\n",
       " 0.03053811751306057,\n",
       " -0.024993976578116417,\n",
       " 0.02148621901869774,\n",
       " 0.01763981208205223,\n",
       " 0.05313889682292938,\n",
       " 0.013485008850693703,\n",
       " -0.023225992918014526,\n",
       " -0.021403977647423744,\n",
       " 0.0260753370821476,\n",
       " 0.0020291954278945923,\n",
       " 0.12753745913505554,\n",
       " 0.0831683874130249,\n",
       " 0.04408949986100197,\n",
       " -0.026703616604208946,\n",
       " 0.005522036459296942,\n",
       " -0.009294891729950905,\n",
       " 0.02007431723177433,\n",
       " -0.09684176743030548,\n",
       " -0.024703947827219963,\n",
       " 0.025086989626288414,\n",
       " 0.002088621724396944,\n",
       " -0.044894054532051086,\n",
       " -0.07861138135194778,\n",
       " -0.004376340191811323,\n",
       " -0.06590451300144196,\n",
       " 0.014689394272863865,\n",
       " -0.057641856372356415,\n",
       " -0.07152029126882553,\n",
       " -0.062326494604349136,\n",
       " 0.003431659657508135,\n",
       " -0.046065520495176315,\n",
       " 0.04530085250735283,\n",
       " -0.026762276887893677,\n",
       " 0.034010931849479675,\n",
       " 0.04547382518649101,\n",
       " -0.028179243206977844,\n",
       " 0.005011788569390774,\n",
       " 0.009630809538066387,\n",
       " -0.030305609107017517,\n",
       " -0.03612480312585831,\n",
       " -0.013626961968839169,\n",
       " -0.032653726637363434,\n",
       " -0.044677503407001495,\n",
       " 0.010642158798873425,\n",
       " -0.027486342936754227,\n",
       " -0.024565109983086586,\n",
       " -0.024747753515839577,\n",
       " 0.053619615733623505,\n",
       " 0.0207899808883667,\n",
       " 0.019468510523438454,\n",
       " 0.053241219371557236,\n",
       " -0.014002465642988682,\n",
       " 0.021243276074528694,\n",
       " -0.04957326129078865,\n",
       " -0.00852261669933796,\n",
       " 0.007852856069803238,\n",
       " -0.057193923741579056,\n",
       " -0.027550650760531425,\n",
       " 0.0053008501417934895,\n",
       " 0.040072932839393616,\n",
       " 0.019597891718149185,\n",
       " -0.045197345316410065,\n",
       " 0.03243584558367729,\n",
       " -0.012342389672994614,\n",
       " 0.03431442007422447,\n",
       " 0.021102111786603928,\n",
       " 0.039846453815698624,\n",
       " 0.03166377916932106,\n",
       " -0.03359024226665497,\n",
       " 0.031647831201553345,\n",
       " -0.00330451806075871,\n",
       " 0.004641843028366566,\n",
       " 0.03758930414915085,\n",
       " -0.05924459546804428,\n",
       " 0.0070283859968185425,\n",
       " 0.0038087156135588884,\n",
       " -0.02578889951109886,\n",
       " -0.021203337237238884,\n",
       " 0.02269122749567032,\n",
       " -0.021772990003228188,\n",
       " -0.27963775396347046,\n",
       " 0.007267452776432037,\n",
       " 0.021072018891572952,\n",
       " 0.04519746080040932,\n",
       " -0.020534515380859375,\n",
       " 0.02431372180581093,\n",
       " -0.0006136888405308127,\n",
       " -0.011857032775878906,\n",
       " -0.032967790961265564,\n",
       " 0.0358431339263916,\n",
       " 0.031281713396310806,\n",
       " 0.06373954564332962,\n",
       " 0.046547841280698776,\n",
       " -0.014470530673861504,\n",
       " 0.01586974412202835,\n",
       " 0.033971283584833145,\n",
       " 0.018059566617012024,\n",
       " 0.002298753010109067,\n",
       " 0.016549894586205482,\n",
       " -0.021714871749281883,\n",
       " -0.034859977662563324,\n",
       " -0.000864923931658268,\n",
       " 0.15126043558120728,\n",
       " -0.02453676424920559,\n",
       " 0.030671197921037674,\n",
       " -0.007318197749555111,\n",
       " -0.006135465577244759,\n",
       " 0.06415146589279175,\n",
       " 0.016021504998207092,\n",
       " -0.03636445850133896,\n",
       " 0.01989860087633133,\n",
       " -0.021172359585762024,\n",
       " 0.04829414188861847,\n",
       " -0.04478098824620247,\n",
       " 0.0476338192820549,\n",
       " 0.000774923712015152,\n",
       " -0.00592794781550765,\n",
       " 0.061542704701423645,\n",
       " 0.023968402296304703,\n",
       " 0.013305027037858963,\n",
       " 0.022684495896100998,\n",
       " 0.014538075774908066,\n",
       " -0.05215906351804733,\n",
       " -0.032749664038419724,\n",
       " 0.08583345264196396,\n",
       " -0.003724843729287386,\n",
       " 0.0013494258746504784,\n",
       " 0.04091989994049072,\n",
       " 0.01165966596454382,\n",
       " 0.058436207473278046,\n",
       " -0.02228615991771221,\n",
       " -0.01152071077376604,\n",
       " 0.004705711267888546,\n",
       " 0.0471826046705246,\n",
       " -0.0019178659422323108,\n",
       " 0.0330093689262867,\n",
       " -0.03505057096481323,\n",
       " -0.020736565813422203,\n",
       " -0.009222205728292465,\n",
       " 0.014618285931646824,\n",
       " 0.006456053350120783,\n",
       " 0.001097834319807589,\n",
       " 0.010223998688161373,\n",
       " 0.08537217974662781,\n",
       " 0.038839515298604965]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
